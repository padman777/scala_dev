


object sparkprac {
  def main(args: Array[String]): Unit = {
//    val sc=new SparkContext("local[*]","sparkrdd")
//        val rdd1=sc.textFile("C:/Users/padma/Downloads/data5.txt")
//        val rdd2=rdd1.flatMap(x=>x.split(" "))
//        val rdd3=rdd2.map(x=>(x,1))
//        val rdd4=rdd3.reduceByKey((x,y)=>x+y)
//       val rdd5 = rdd4.sortBy(x=>x._1)
////    rdd4.collect().foreach(println)
//    rdd5.collect().foreach(println)
//    rdd5.take(2).foreach(println)
//    val a = (10, 20, 30, 40)
//    print(a._1)
//    val data = Array(1, 2, 3, 4, 5)
//    val input = sc.parallelize(data)
//    input.saveAsTextFile("C:/Users/padma/Downloads/sep16")

//  val a = Array(10,20,30,40,50)
//
//  val rdd = sc.parallelize(a)
//
//  rdd.collect().foreach(println)
//    scala.io.StdIn.readLine()

//    val df = spark.read
//      .format("csv")
//      .option("header",true)
//      .option("path","C:/Users/padma/Downloads/details.csv")



  }

}
